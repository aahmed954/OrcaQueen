# Oracle ARM Node - Orchestration Services (ARM64 COMPATIBLE)
# Deploy to: 100.96.197.84
# FIXED: ARM compatibility issues resolved

version: '3.8'

services:
  # PostgreSQL - ARM64 Compatible
  postgres:
    image: postgres:15-alpine
    platform: linux/arm64  # Explicit ARM64
    container_name: oracle-postgres
    restart: always
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: litellm
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-securepass123}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - aiswarm
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U litellm"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis - ARM64 Compatible  
  redis:
    image: redis:7-alpine
    platform: linux/arm64  # Explicit ARM64
    container_name: oracle-redis
    restart: always
    command: >
      redis-server
      --appendonly yes
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - aiswarm
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # LiteLLM - NEEDS VERIFICATION OR ARM BUILD
  # TODO: Test if ghcr.io/berriai/litellm supports ARM64
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    platform: linux/arm64  # May fail - test required
    container_name: oracle-litellm
    restart: always
    ports:
      - "4000:4000"
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - DATABASE_URL=postgresql://litellm:${POSTGRES_PASSWORD:-securepass123}@postgres:5432/litellm
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - STORE_MODEL_IN_DB=true
      - LITELLM_MODE=PRODUCTION
      - CUSTOM_API_BASE=http://100.72.73.3:8000/v1  # Starlord vLLM
    volumes:
      - ./config/litellm.yaml:/app/config.yaml:ro
      - litellm_logs:/app/logs
    networks:
      - aiswarm
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Open WebUI - ARM64 Alternative if official doesn't support
  open-webui:
    # OPTION 1: Try official ARM build
    image: ghcr.io/open-webui/open-webui:main
    platform: linux/arm64
    # OPTION 2: If fails, use Node.js base and build custom
    # build:
    #   context: ../custom-builds/open-webui-arm64
    #   platform: linux/arm64
    container_name: oracle-open-webui
    restart: always
    ports:
      - "3000:8080"
    environment:
      - WEBUI_NAME=AI Swarm Control Center
      - WEBUI_URL=http://100.96.197.84:3000
      - OPENAI_API_BASE_URL=http://litellm:4000/v1
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
      - ENABLE_COMMUNITY_SHARING=false
      - ENABLE_ADMIN_EXPORT=true
      - DEFAULT_MODELS=llama-3.2-dark-champion,dolphin-3-llama-3.1-70b,deepseek-v3
    volumes:
      - open_webui_data:/app/backend/data
    networks:
      - aiswarm
    depends_on:
      litellm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # CPU-Only Inference Server (Alternative to vLLM for ARM)
  cpu-inference:
    image: python:3.11-slim
    platform: linux/arm64
    container_name: oracle-cpu-inference
    restart: always
    ports:
      - "8001:8000"  # Different port to avoid conflict
    environment:
      - PYTHONUNBUFFERED=1
      - MODEL_NAME=microsoft/DialoGPT-small  # Lightweight model for CPU
      - MAX_LENGTH=512
      - DEVICE=cpu
    volumes:
      - ./scripts/cpu_inference_server.py:/app/server.py:ro
      - cpu_model_cache:/root/.cache/huggingface
    working_dir: /app
    command: |
      bash -c "
      pip install transformers torch flask requests huggingface-hub && 
      python server.py
      "
    networks:
      - aiswarm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3

  # Pipelines - Node.js based, should support ARM64
  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    platform: linux/arm64
    container_name: oracle-pipelines
    restart: always
    ports:
      - "9099:9099"
    environment:
      - PIPELINES_PORT=9099
      - PIPELINES_OPENAI_API_BASE_URL=http://litellm:4000/v1
      - PIPELINES_OPENAI_API_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - ./pipelines:/app/pipelines:ro
      - pipelines_data:/app/data
    networks:
      - aiswarm
    depends_on:
      - litellm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9099/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Health Monitor - ARM64 Compatible
  health-monitor:
    image: prom/node-exporter:latest
    platform: linux/arm64
    container_name: oracle-health-monitor
    restart: always
    ports:
      - "9100:9100"
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - aiswarm

  # ARM Performance Monitor
  arm-monitor:
    image: python:3.11-slim
    platform: linux/arm64
    container_name: oracle-arm-monitor
    restart: always
    ports:
      - "9101:9101"
    command: |
      bash -c "
      pip install psutil prometheus-client && 
      cat > /monitor.py << 'EOF'
      import psutil
      import time
      from prometheus_client import start_http_server, Gauge
      
      # ARM-specific metrics
      cpu_temp = Gauge('arm_cpu_temperature_celsius', 'ARM CPU temperature')
      cpu_freq = Gauge('arm_cpu_frequency_mhz', 'ARM CPU frequency')
      memory_usage = Gauge('arm_memory_usage_percent', 'ARM memory usage')
      
      start_http_server(9101)
      print('ARM Performance Monitor Started')
      
      while True:
          try:
              # CPU temperature (ARM-specific)
              try:
                  temps = psutil.sensors_temperatures()
                  if 'cpu_thermal' in temps:
                      cpu_temp.set(temps['cpu_thermal'][0].current)
              except:
                  pass
              
              # CPU frequency
              freq = psutil.cpu_freq()
              if freq:
                  cpu_freq.set(freq.current)
              
              # Memory usage
              memory = psutil.virtual_memory()
              memory_usage.set(memory.percent)
              
              print(f'ARM CPU: {freq.current if freq else 0}MHz | Memory: {memory.percent}%')
              time.sleep(30)
          except Exception as e:
              print(f'Monitoring error: {e}')
              time.sleep(60)
      EOF
      python /monitor.py
      "
    networks:
      - aiswarm

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  litellm_logs:
    driver: local
  open_webui_data:
    driver: local
  pipelines_data:
    driver: local
  cpu_model_cache:
    driver: local

networks:
  aiswarm:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24

# ARM64 DEPLOYMENT NOTES:
# 1. All images explicitly set to linux/arm64 platform
# 2. vLLM replaced with CPU-only inference server for ARM compatibility  
# 3. Added ARM-specific monitoring for temperature/frequency
# 4. PostgreSQL and Redis confirmed ARM64 compatible
# 5. LiteLLM and OpenWebUI need testing - may require custom builds
# 6. Consider building custom ARM64 images if official ones fail